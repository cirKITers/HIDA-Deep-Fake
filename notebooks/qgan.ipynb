{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 12\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "selected_label = 0\n",
    "n_samples = 200\n",
    "n_qubits = 4\n",
    "n_layers = 22\n",
    "noise_gain = torch.pi/8\n",
    "noise_offset = torch.pi/2\n",
    "seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.Resize(image_size), torchvision.transforms.ToTensor()]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lc3267/Documents/CodeWorkspace/HIDA-Deep-Fake/.venv/lib64/python3.11/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = (dataset.train_labels==selected_label).nonzero().flatten()\n",
    "\n",
    "n_figures = 8\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, zidx in enumerate(images[:n_figures]):\n",
    "    image = dataset[zidx][0].reshape(image_size,image_size)\n",
    "    plt.subplot(1,n_figures,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid(domain, samples):\n",
    "    tensors = tuple(2 * [torch.linspace(domain[0], domain[1], steps=samples)])\n",
    "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
    "    return mgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, n_samples):\n",
    "        self.z = torch.stack([dataset[zidx][0][0] for zidx in images[:n_samples]]) # first [0] discards label, second [0] discards color channel\n",
    "        self.x = torch.stack([generate_grid([-torch.pi/2, torch.pi/2], image_size) for _ in range(self.z.shape[0])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.z)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.z[idx], self.x[idx]\n",
    "\n",
    "gan_dataset = Dataset(images, n_samples)\n",
    "dataloader = torch.utils.data.DataLoader(gan_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Inputs to first hidden layer (num_input_features -> 64)\n",
    "            nn.Linear(image_size * image_size, 64),\n",
    "            nn.ReLU(),\n",
    "            # First hidden layer (64 -> 16)\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            # Second hidden layer (16 -> output)\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseSource(nn.Module):\n",
    "\n",
    "    def __init__(self, output_shape, seed, noise_gain, noise_offset, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.rng = torch.Generator().manual_seed(seed)\n",
    "        self.output_shape = output_shape\n",
    "        self.noise_gain = noise_gain\n",
    "        self.noise_offset = noise_offset\n",
    "\n",
    "    def forward(self):\n",
    "        return self.noise_offset + torch.randn(self.output_shape, generator=self.rng) * self.noise_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_qubits, n_layers, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers + 1\n",
    "\n",
    "        dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        self.qnode = qml.QNode(self.circuit, dev, interface=\"torch\")\n",
    "        self.qlayer = qml.qnn.TorchLayer(self.qnode, {\"weights\": [self.n_layers, self.n_qubits, self.vqc(None)]})\n",
    "\n",
    "\n",
    "    def circuit(self, weights, inputs):\n",
    "        x = inputs[:, :2]   # [B*IS*IS, NQ+2] -> [B*IS*IS, 2]\n",
    "        p = inputs[:, 2:]   # [B*IS*IS, NQ+2] -> [B*IS*IS, NQ]\n",
    "        \n",
    "        self.nec(p) # prepare random states\n",
    "\n",
    "        # build the trainable circuit\n",
    "        for layer in range(self.n_layers-1):\n",
    "            self.vqc(weights[layer])\n",
    "            self.iec(x)\n",
    "\n",
    "        # add a last vqc layer\n",
    "        self.vqc(weights[-1])\n",
    "\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "    def nec(self, p):\n",
    "        for qubit in range(self.n_qubits):\n",
    "            qml.RY(p[:, qubit], wires=qubit)\n",
    "\n",
    "    def iec(self, x):\n",
    "        for qubit in range(self.n_qubits):\n",
    "            qml.RY(x[:, 0], wires=qubit)\n",
    "            qml.RX(x[:, 1], wires=qubit)\n",
    "\n",
    "    def vqc(self, weights):\n",
    "\n",
    "        if weights is None:\n",
    "            return 3\n",
    "        for qubit, qubit_weights in enumerate(weights):\n",
    "            qml.RX(qubit_weights[0], wires=qubit)\n",
    "            qml.RZ(qubit_weights[1], wires=qubit)\n",
    "\n",
    "        for qubit, qubit_weights in enumerate(weights):\n",
    "            qml.CRX(\n",
    "                qubit_weights[2],\n",
    "                wires=[\n",
    "                    weights.shape[0] - qubit - 1,\n",
    "                    (weights.shape[0] - qubit) % weights.shape[0],\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    def forward(self, p, x):\n",
    "        # get the known variables\n",
    "        batch_size = x.shape[0]\n",
    "        image_sidelength = x.shape[1]\n",
    "        \n",
    "        x_in = x.reshape(batch_size, -1, 2) # [B, IS, IS, 2] -> [B, IS*IS, 2]\n",
    "        p_in = p.repeat(batch_size* image_sidelength * image_sidelength)   # [NQ] -> [B*IS*IS*NQ]\n",
    "        p_in = p_in.reshape(batch_size,image_sidelength*image_sidelength, self.n_qubits) # [B*IS*IS*NQ] -> [B, IS*IS, NQ]\n",
    "        combined = torch.cat((x_in, p_in), dim=2) # [B, IS*IS, 2] + [B, IS*IS, NQ] -> [B, IS*IS, NQ+2]\n",
    "        z = self.qlayer(combined)\n",
    "        z = (z +1) /2 # move into range [0,1]\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_source = NoiseSource((n_qubits), seed=seed, noise_gain=noise_gain, noise_offset=noise_offset)\n",
    "generator = Generator(n_qubits=n_qubits, n_layers=n_layers)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_discriminator = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
    "opt_generator = torch.optim.Adam(generator.parameters(), lr=0.1)\n",
    "\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started. Navigate to the MLflow UI at http://localhost:5000/#/experiments/251055496897115745/runs/491d4fe6adb1418ab255ae18c5f62cad\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "mlflow.set_experiment('QGAN Test')\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_param(\"image_size\", image_size)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"selected_label\", selected_label)\n",
    "    mlflow.log_param(\"n_samples\", n_samples)\n",
    "    mlflow.log_param(\"n_qubits\", n_qubits)\n",
    "    mlflow.log_param(\"n_layers\", n_layers)\n",
    "    mlflow.log_param(\"noise_gain\", noise_gain)\n",
    "    mlflow.log_param(\"noise_offset\", noise_offset)\n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "\n",
    "    print(f\"Training started. Navigate to the MLflow UI at http://localhost:5000/#/experiments/{run.info.experiment_id}/runs/{run.info.run_id}\")\n",
    "\n",
    "    # Sample all the noise beforehand so have the same each epoch\n",
    "    all_p_hat = []\n",
    "    for i, (z, x) in enumerate(dataloader):\n",
    "        all_p_hat.append(noise_source())\n",
    "    all_p_hat = torch.stack(all_p_hat)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i, (z, x) in enumerate(dataloader):\n",
    "\n",
    "            z_hat = generator(all_p_hat[i], x)\n",
    "\n",
    "            y_hat = discriminator(z_hat.detach())\n",
    "            y = discriminator(z.view(-1, image_size * image_size))\n",
    "\n",
    "            disc_loss_real = loss(y, torch.ones_like(y))\n",
    "            disc_loss_fake = loss(y_hat, torch.zeros_like(y_hat))\n",
    "\n",
    "            opt_discriminator.zero_grad()\n",
    "            disc_loss_real.backward()\n",
    "            disc_loss_fake.backward()\n",
    "            opt_discriminator.step()\n",
    "\n",
    "            opt_generator.zero_grad()\n",
    "            y_hat = discriminator(z_hat)\n",
    "            gen_loss_fake = loss(y_hat, torch.ones_like(y_hat))\n",
    "            gen_loss_fake.backward()\n",
    "            opt_generator.step()\n",
    "\n",
    "            mlflow.log_metric(\"discriminator_loss\", disc_loss_real.item() + disc_loss_fake.item())\n",
    "            mlflow.log_metric(\"generator_loss\", gen_loss_fake.item())\n",
    "\n",
    "        preds = z_hat.reshape(-1, image_size, image_size).detach().cpu().numpy()\n",
    "        for i in range(min(n_figures, batch_size)):\n",
    "            mlflow.log_image(preds[i], f\"generated_image_{i}_epoch_{epoch}.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
